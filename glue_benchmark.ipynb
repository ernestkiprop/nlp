{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting typing_extensions\n",
      "  Downloading typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Downloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Installing collected packages: typing_extensions\n",
      "Successfully installed typing_extensions-4.12.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: C:\\Users\\Ernest\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Install required libraries\n",
    "!pip install --upgrade pip\n",
    "!pip install --upgrade typing_extensions\n",
    "!pip install wandb\n",
    "!pip install transformers datasets wandb evaluate pynvml scipy scikit-learn transformers[torch] accelerate>=0.26.0\n",
    "!pip install --upgrade transformers\n",
    "\n",
    "!pip install transformers datasets wandb evaluate pynvml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os._exit(00) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, wandb\n",
    "os.environ[\"WANDB_API_KEY\"] = \"b3c53303f76ce37498666700f945f85807f5db44\"\n",
    "wandb.login()\n",
    "\n",
    "\n",
    "# Import necessary libraries\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorWithPadding,\n",
    "    TrainerCallback,\n",
    ")\n",
    "import numpy as np\n",
    "import evaluate\n",
    "import time\n",
    "from pynvml import nvmlInit, nvmlDeviceGetHandleByIndex, nvmlDeviceGetMemoryInfo\n",
    "\n",
    "# Initialize wandb\n",
    "wandb.login()\n",
    "\n",
    "# Function to log GPU memory usage\n",
    "def log_gpu_memory():\n",
    "    try:\n",
    "        nvmlInit()\n",
    "        handle = nvmlDeviceGetHandleByIndex(0)  # GPU index 0\n",
    "        info = nvmlDeviceGetMemoryInfo(handle)\n",
    "        gpu_memory = {\n",
    "            \"memory_used_MB\": info.used // 1024**2,\n",
    "            \"memory_free_MB\": (info.total - info.used) // 1024**2,\n",
    "            \"memory_total_MB\": info.total // 1024**2,\n",
    "        }\n",
    "        wandb.log(gpu_memory)\n",
    "    except Exception as e:\n",
    "        print(f\"Error logging GPU memory: {e}\")\n",
    "\n",
    "# Custom callback to log memory and time during training\n",
    "class MemoryTimeCallback(TrainerCallback):\n",
    "    def __init__(self):\n",
    "        self.start_time = None\n",
    "\n",
    "    def on_init_end(self, args, state, control, **kwargs):\n",
    "        print(\"Trainer initialization complete.\")\n",
    "\n",
    "    def on_train_begin(self, args, state, control, **kwargs):\n",
    "        self.start_time = time.time()\n",
    "        print(\"Training started.\")\n",
    "\n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        elapsed_time = time.time() - self.start_time  # Time since training began\n",
    "\n",
    "        # Log GPU memory usage if a GPU is available\n",
    "        if torch.cuda.is_available():\n",
    "            log_gpu_memory()\n",
    "\n",
    "        # Log elapsed time per step\n",
    "        wandb.log({\"train/step_time_seconds\": elapsed_time}, step=state.global_step)\n",
    "\n",
    "    def on_train_end(self, args, state, control, **kwargs):\n",
    "        print(\"Training completed.\")\n",
    "\n",
    "# Mapping of GLUE tasks to their input keys\n",
    "task_to_keys = {\n",
    "    \"cola\": (\"sentence\", None),\n",
    "    \"sst2\": (\"sentence\", None),\n",
    "    \"mrpc\": (\"sentence1\", \"sentence2\"),\n",
    "    \"qqp\": (\"question1\", \"question2\"),\n",
    "    \"stsb\": (\"sentence1\", \"sentence2\"),\n",
    "    \"mnli\": (\"premise\", \"hypothesis\"),\n",
    "    \"qnli\": (\"question\", \"sentence\"),\n",
    "    \"rte\": (\"sentence1\", \"sentence2\"),\n",
    "    \"wnli\": (\"sentence1\", \"sentence2\"),\n",
    "}\n",
    "\n",
    "# List of all GLUE tasks (excluding 'ax' since it's test-only)\n",
    "glue_tasks = list(task_to_keys.keys())\n",
    "\n",
    "# Loop through each task in the GLUE benchmark\n",
    "for task in glue_tasks:\n",
    "    print(f\"Running GLUE task: {task}\")\n",
    "    \n",
    "    # Load dataset for the current task\n",
    "    dataset = load_dataset(\"glue\", task)\n",
    "    \n",
    "    # Initialize tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    \n",
    "    # Get the appropriate keys for the current task\n",
    "    sentence1_key, sentence2_key = task_to_keys[task]\n",
    "    \n",
    "    # Tokenize the dataset dynamically based on the task's input keys\n",
    "    def tokenize_function(examples):\n",
    "        if sentence2_key is None:  # Single-sentence tasks (e.g., cola, sst2)\n",
    "            return tokenizer(examples[sentence1_key], truncation=True)\n",
    "        else:  # Sentence-pair tasks (e.g., mrpc, qqp)\n",
    "            return tokenizer(examples[sentence1_key], examples[sentence2_key], truncation=True)\n",
    "    \n",
    "    tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "    \n",
    "    # Set format for PyTorch tensors\n",
    "    tokenized_datasets.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "    \n",
    "    # Load pre-trained model for sequence classification\n",
    "    num_labels = len(dataset[\"train\"].features[\"label\"].names) if task != \"stsb\" else 1  # Regression for STS-B\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=num_labels)\n",
    "    \n",
    "    # Define evaluation metric(s)\n",
    "    metric = evaluate.load(\"glue\", task)\n",
    "    \n",
    "    def compute_metrics(eval_pred):\n",
    "        logits, labels = eval_pred\n",
    "        if task == \"stsb\":  # For STS-B regression, use Pearson/Spearman correlation\n",
    "            predictions = logits[:, 0]\n",
    "        else:  # For classification tasks, use argmax for predictions\n",
    "            predictions = np.argmax(logits, axis=-1)\n",
    "        return metric.compute(predictions=predictions, references=labels)\n",
    "    \n",
    "    # Data collator to dynamically pad batches\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "    \n",
    "    # Define training arguments with wandb integration and logging steps for memory/time tracking\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f\"./{task}-results\",\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=32,\n",
    "        per_device_eval_batch_size=32,\n",
    "        num_train_epochs=1,\n",
    "        weight_decay=0.01,\n",
    "        save_total_limit=1, \n",
    "        logging_steps=200,\n",
    "        report_to=\"wandb\",\n",
    "        run_name=f\"bert-{task}\"\n",
    "    )\n",
    "    \n",
    "    # Instantiate the custom callback for memory/time logging\n",
    "    memory_time_callback = MemoryTimeCallback()\n",
    "    \n",
    "    # Initialize Trainer object with the custom callback integrated\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_datasets[\"train\"],\n",
    "        eval_dataset=tokenized_datasets[\"validation\"],\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[memory_time_callback],\n",
    "    )\n",
    "    \n",
    "    # Start training (metrics are automatically logged to wandb)\n",
    "    trainer.train()\n",
    "\n",
    "# Finish the wandb run when all tasks are done (optional)\n",
    "wandb.finish()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
